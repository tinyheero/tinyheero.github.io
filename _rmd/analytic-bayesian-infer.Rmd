---
title: "The Analytical Approach to Bayesian Inference"
date: "March 8th, 2017"
layout: post
output:
  html_document
tags: [R, stats, bayesian]
---

```{r message = FALSE, echo = FALSE}
library("knitr")
library("cowplot")
library("Cairo")
library("ggplot2")
library("dplyr")

knitr::opts_chunk$set(fig.path="{{ site.url }}/assets/analytic-bayesian-infer/",
                      dev = "svg")
#                      fig.ext = ".svg")
theme_set(theme_grey())
```

In a previous post called "[How to Do Bayesian Inference 101](% 2017-03-08-how-to-bayesian-infer-101 %})", I introduced and walked through the key steps to Bayesian inference using a coin flipping example. In that post, I demonstrated an exhaustive approach to calculating the posterior distribution. In practice though, this approach will not scale well with a larger parameter space. As such, there are two main approaches to calculating the posterior distribution in Bayesian inference:

1. Analytical approach. 
1. Approximation approach (e.g. using Markov chain Monte Carlo (MCMC) methods).

The analytical approach will be the topic of today's post and we shall save the approximation approach for a different post. When I was first learning this, I found it a bit confusing what this meant. What helped me was first understanding this in the context of why Bayesian inference is challenging. Once we understand this, it will be clear when we can and cannot use the analytical approach. Then we will walk through the approach using the [coin flipping example]((% 2017-03-08-how-to-bayesian-infer-101#the-coin-flipping-example %}).

Here is an overview of what will be discussed in this post.

**Table of Contents**

<ul data-toc="body" data-toc-headings="h2,h3"></ul>

## What Makes Bayesian Inference Hard?

When using an analytical approach, what we are doing is solving the posterior in a pure mathematical way. This is often referred to as a "closed-form" solution and is actually the "ideal" way of calculating the posterior.

* Integral 

## Beta Prior Distribution

To do Bayesian statistics in a purely mathematical way, we need a way to describe the prior distribution such that we have a prior probability for each value of $\theta$ in the interval [0, 1]. In theory, we could actually use any probability distribution that gives values between the interval [0, 1]. However, there are 2 important considerations:

1. The product of $P(D\ |\ \theta)$ and $P(\theta)$ (i.e. the numerator of Bayes' rule) should result in a function that has the same form as $P(\theta)$. The reason why we want this is because we can subsequently include additional data and derive another posterior distribution, which is the same form as the prior.
1. The denominator of Bayes' rule, $P(D)$, needs to be solvable analytically. This depends on how $P(\theta)$ relates to $P(D | \theta)$.

When $P(D\ |\ \theta)$ and $P(\theta)$ combine to give a form that is the same as the prior distribution, we call $P(\theta)$ a **conjugate prior** for $P(D | \theta)$. In the context of the binomial distribution being the likelihood function, the corresponding conjugate prior is the **beta distribution**. Formally, the beta distribution can be described as follows:

$$
p(\theta | a, b) = beta(\theta | a,b) = \frac{\theta^{(a-1)}(1 - \theta)^{(b-1)}}{B(a,b)}
$$

Where $B(a,b)$ is a normalizing constant that simply ensures that the area under the beta density integrates to 1. I won't go into too much detail about the beta distribution and instead refer you to this wonderful post by David Robinson titled "[Understanding the beta distribution (using baseball statistics)](http://varianceexplained.org/statistics/beta_distribution_and_baseball/) for more initution. To steal a quote:

> In short, the beta distribution can be understood as representing a probability distribution of probabilities- that is, it represents all the possible values of a probability when we donâ€™t know what that probability is.

The beta distribution is parameterized by two shape parameters, $a$ and $b$, that can take on any real positive value. Below are a few examples of some beta distributions.

```{r beta-distr-examples}
probs <- seq(0, 1, len = 100)

beta.p.tpl <- 
  data.frame(x = probs) %>%
  ggplot(aes(x = probs)) +
  ylab("Density of Beta") +
  xlab("Probability")

p1 <- beta.p.tpl + 
  stat_function(fun = dbeta, args = list(shape1 = 0.1, shape2 = 0.1)) +
  ggtitle("a = 0.1, b = 0.1")

p2 <- beta.p.tpl + 
  stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 1)) +
  ggtitle("a = 1, b = 1")

p3 <- beta.p.tpl + 
  stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2)) +
  ggtitle("a = 2, b = 2")

p4 <- beta.p.tpl + 
  stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 4)) +
  ggtitle("a = 2, b = 4")

plot_grid(p1, p2, p3, p4, nrow = 2, ncol = 2, labels = LETTERS[1:4])
```

Here we have 4 different beta distributions corresponding to different shape parameters. You can see how each plot shows a different distribution of probabilities:

* Panel A shows a distribution that places a lot of weight towards 0 and 1 probabilites. 
* Panel B shows a flat distribution demonstrating that the prior probability for each probability is equally likely. This often refer to as a "non-informative" prior.
* Panel C shows a distribution that places weight towards the middle probabilities.
* Panel D shows a skewed distribution with weight towards the lowest probability value.

In terms of choosing these the values for these shape parameters, essentially what we want to do is convert our prior belief into the shape parameter values such that our beta distribution will appropriately represent our prior belief. What we can do is treat $a$ as the number of heads and $b$ as the number of tails. 

TODO: need to fill this in.

## The Posterior Beta

Now that we have the conjugate prior selected (i.e. beta distribution), let's see how the posterior distribution looks when we combine our Binomial likelihood with the beta prior. If we let our data be N flips and z heads, then after some math our Bayes' rule looks like this:

<div>
$$\begin{align}
P(\theta\ |\ z, N) &= \frac{P(z, N | \theta) P(\theta)}{P(z,N)} \\
P(\theta\ |\ z, N) &= \theta^{((z+a)-1)}(1-\theta)^{((N-z+b)-1} / B(z + a, N - z + b)
\end{align}$$
</div>

## References

* [Understanding the beta distribution (using baseball statistics)](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)
* [Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan](https://sites.google.com/site/doingbayesiandataanalysis/)
* [Points of significance: Bayesian statistics](www.nature.com/nmeth/journal/v12/n5/full/nmeth.3368.html)

## R Session

```{r session, echo = FALSE}
devtools::session_info()
```
